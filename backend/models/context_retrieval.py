# -*- coding: utf-8 -*-
"""Context_retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-htsZL14EduLzdqeN9aIYb-LAuLzeL5q
"""

import json
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np


import json
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

import os

# Get the absolute path to the JSON file
file_path = os.path.join(os.path.dirname(__file__), "physio_fixed.json")

with open(file_path) as f:
    squad_data = json.load(f)

# Extract unique contexts
contexts = []
for item in squad_data['data']:
    for paragraph in item['paragraphs']:
        context = paragraph['context']
        contexts.append(context)

contexts = list(set(contexts))  # optional deduplication

# Embed the contexts
embedder = SentenceTransformer("all-MiniLM-L6-v2")  # Can be replaced with BioBERT
context_embeddings = embedder.encode(contexts, convert_to_numpy=True)

# Index using FAISS
dimension = context_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(context_embeddings)

# Save contexts for retrieval
with open("doc_store.json", "w") as f:
    json.dump(contexts, f)

def get_top_k_contexts(question, k=1):
    question_embedding = embedder.encode([question])
    D, I = index.search(np.array(question_embedding), k)
    with open("doc_store.json", "r") as f:
        docs = json.load(f)
    return [docs[i] for i in I[0]]

from transformers import AutoTokenizer, AutoModelForQuestionAnswering, PreTrainedModel, PretrainedConfig
from safetensors.torch import load_file
import torch

# Load config and tokenizer from the base model
model_path = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"
tokenizer = AutoTokenizer.from_pretrained(model_path)
config = PretrainedConfig.from_pretrained(model_path)

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from safetensors.torch import load_file

# Path to folder containing config.json, tokenizer, etc.
model_dir = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"

# Load tokenizer and base model architecture
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForQuestionAnswering.from_pretrained(model_dir, local_files_only=False)

# Get the absolute path to the model file
model_path = os.path.join(os.path.dirname(__file__), "model.safetensors")

# Check if the file exists
if not os.path.exists(model_path):
    raise FileNotFoundError(f"Model file not found: {model_path}")

# Load the model weights
try:
    state_dict = load_file(model_path)
    model.load_state_dict(state_dict, strict=False)  # Allow missing keys
    model.eval()
    print("Model loaded successfully with missing keys!")
except Exception as e:
    raise RuntimeError(f"Failed to load model weights: {e}")

def answer_question(question):
    context = get_top_k_contexts(question, k=1)[0]
    inputs = tokenizer.encode_plus(question, context, return_tensors="pt", truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)

    start_idx = torch.argmax(outputs.start_logits)
    end_idx = torch.argmax(outputs.end_logits) + 1

    input_ids = inputs["input_ids"][0]
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start_idx:end_idx]))

    return answer.strip(), context

question = "my head hurts, what therapy do i do"
answer, ctx = answer_question(question)

print("Question:", question)
print("Answer:", answer)
print("Retrieved Context:", ctx)

