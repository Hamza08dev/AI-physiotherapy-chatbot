# -*- coding: utf-8 -*-
"""Context_retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-htsZL14EduLzdqeN9aIYb-LAuLzeL5q
"""

import json
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers.utils import logging
logging.set_verbosity_error() 

import json
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

import os

# Get the absolute path to the JSON file
file_path = os.path.join(os.path.dirname(__file__), "physio_fixed.json")

with open(file_path) as f:
    squad_data = json.load(f)

# Extract unique contexts
contexts = []
for item in squad_data['data']:
    for paragraph in item['paragraphs']:
        context = paragraph['context']
        contexts.append(context)

contexts = list(set(contexts))  # optional deduplication

# Embed the contexts
embedder = SentenceTransformer("all-MiniLM-L6-v2")  # Can be replaced with BioBERT
context_embeddings = embedder.encode(contexts, convert_to_numpy=True)

# Index using FAISS
dimension = context_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(context_embeddings)

# Save contexts for retrieval
with open("doc_store.json", "w") as f:
    json.dump(contexts, f)

def get_top_k_contexts(question, k=1):
    question_embedding = embedder.encode([question])
    D, I = index.search(np.array(question_embedding), k)
    with open("doc_store.json", "r") as f:
        docs = json.load(f)
    return [docs[i] for i in I[0]]

from transformers import AutoTokenizer, AutoModelForQuestionAnswering, PreTrainedModel, PretrainedConfig
from safetensors.torch import load_file
import torch

# Load config and tokenizer from the base model
model_path = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"
tokenizer = AutoTokenizer.from_pretrained(model_path)
config = PretrainedConfig.from_pretrained(model_path)

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from safetensors.torch import load_file

# Path to folder containing config.json, tokenizer, etc.
model_dir = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"

# Load tokenizer and base model architecture
tokenizer = AutoTokenizer.from_pretrained(model_dir)


# Get the absolute path to the model file
model_path = os.path.join(os.path.dirname(__file__), "model.safetensors")
model = AutoModelForQuestionAnswering.from_pretrained(model_dir, local_files_only=False)
# Check if the file exists
if not os.path.exists(model_path):
    raise FileNotFoundError(f"Model file not found: {model_path}")

# Load the model weights
try:
    model.load_state_dict(load_file(model_path))
    model.eval()
    print("Model loaded successfully!")
except Exception as e:
    raise RuntimeError(f"Failed to load model weights: {e}")

def answer_question(question):
    context = get_top_k_contexts(question, k=1)[0]

    # Tokenize the inputs (include offset_mapping for later use)
    inputs = tokenizer(
        question,
        context,
        return_tensors="pt",
        return_offsets_mapping=True,  # Only for later use, not to be passed to the model
        truncation=True,
        max_length=512
    )

    # Extract offset_mapping if you want to use it later (not for model input)
    offset_mapping = inputs["offset_mapping"][0]  # Extracting the offset mapping

    # Now pass only the tensors required for the model
    inputs_for_model = {key: value for key, value in inputs.items() if key != "offset_mapping"}

    with torch.no_grad():
        # Pass the tokenized inputs to the model (without offset_mapping)
        outputs = model(**inputs_for_model)

    start_idx = torch.argmax(outputs.start_logits)
    end_idx = torch.argmax(outputs.end_logits)

    input_ids = inputs["input_ids"][0]

    if end_idx < start_idx:
        return "[Could not extract a valid answer]", context

    # Decode tokens normally
    answer = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)

    return answer.strip(), context




question = "my head hurts, what therapy do i do"
answer, ctx = answer_question(question)

print("Question:", question)
print("Answer:", answer)
print("Retrieved Context:", ctx)

